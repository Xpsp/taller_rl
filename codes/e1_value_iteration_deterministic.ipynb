{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteración por valor\n",
    "Algoritmo desarrollado alrededor de los años 60's que usa la ecuación de optimalidad de Bellman para obtener la política óptima.\n",
    "\n",
    "## Ecuación de optimalidad de Bellman\n",
    "Propuesta en `1957` por Richard Bellman. En el contexto de los `MDP` proporciona una relación recursiva para calcular el valor de cada estado que permite calcular las políticas óptimas.\n",
    "\n",
    "\\begin{gather}\n",
    "  v^*(s)=\\max_{a}\\sum_{s'}P(s'|s,a)\\left[r+\\gamma v^*(s')\\right]\\\\\n",
    "  \\pi^*(s)=\\arg\\max_{a} v^*(s)\n",
    "\\end{gather}\n",
    "\n",
    "Donde:\n",
    "- $v(s)$ = es el valor del estado s.\n",
    "- $a$ es una acción.\n",
    "- $s'$ es el estado resultante de la acción a\n",
    "- $P(s'| s, a)$ es la probabilidda de que el estado $s'$ sea el resultado de realizar la acción $a$ en el estado $s$.\n",
    "- $r$ es la recompensa de de la acción $a$ en el estado $s$ que resulta en el estado $s'$\n",
    "- $\\gamma$ es el factor de descuento\n",
    "\n",
    "<!-- <div align=\"right\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1qW5SqoblzIFVBeWWxdQ5uXPbxCP_mMzD\"\n",
    "  alt=\"Value Iteration\">\n",
    "</div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema\n",
    "*Obtenido de \"Artificial Intelligence, A modern Approach, Stuart Russell and Peter Norvig\"*\n",
    "\n",
    "Tome en cuenta el siguiente entorno\n",
    "\n",
    "<div align=\"right\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=11_sJaX_lpb8B7vR2YUhuUqxbKdRKd0A2\"\n",
    "  alt=\"Value Iteration\">\n",
    "</div>\n",
    "\n",
    "Las acciones posibles son:\n",
    "- Arriba (\"U\")\n",
    "- Derecha (\"R\")\n",
    "- Abajo (\"D\")\n",
    "- Izquierda (\"L\")\n",
    "\n",
    "Obtenga la política óptima tomando en cuenta un entorno determinístico $P(s', s, a) = 1$, $\\gamma=1$ y una recompensa de -0.04 por moverse.\n",
    "\n",
    "`Pista`: Si en entorno es determinístico $P(s', s, a) = 1$, la ecuación de optimalidad de Bellman se reduce a:\n",
    "\\begin{gather}\n",
    "  v^*(s)=\\max_{a}\\left[r+\\gamma v^*(s')\\right]\\\\\n",
    "  \\pi^*(s)=\\arg\\max_{a} v^*(s)\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3)]\n",
      "\n",
      "Rewards:\n",
      "(0, 0): -0.04\n",
      "(0, 1): -0.04\n",
      "(0, 2): -0.04\n",
      "(0, 3): 1\n",
      "(1, 0): -0.04\n",
      "(1, 2): -0.04\n",
      "(1, 3): -1\n",
      "(2, 0): -0.04\n",
      "(2, 1): -0.04\n",
      "(2, 2): -0.04\n",
      "(2, 3): -0.04\n",
      "\n",
      "Actions:\n",
      "(0, 0): ['R', 'D']\n",
      "(0, 1): ['R', 'L']\n",
      "(0, 2): ['R', 'D', 'L']\n",
      "(1, 0): ['U', 'D']\n",
      "(1, 2): ['U', 'R', 'D']\n",
      "(2, 0): ['U', 'R']\n",
      "(2, 1): ['R', 'L']\n",
      "(2, 2): ['U', 'R', 'L']\n",
      "(2, 3): ['U', 'L']\n"
     ]
    }
   ],
   "source": [
    "# Initial Conditions\n",
    "h, w = 3, 4\n",
    "gamma = 1\n",
    "r = -0.04\n",
    "\n",
    "# Enviroment, actions and rewards\n",
    "walls = [(1,1)]\n",
    "states = [(i,j) for i in range(h) for j in range(w) if (i,j) not in walls]\n",
    "rewards = {s:r for s in states}\n",
    "\n",
    "rewards[(0,3)] = 1\n",
    "rewards[(1,3)] = -1\n",
    "\n",
    "end_actions = [(0,3),(1,3)]\n",
    "\n",
    "actions = {}\n",
    "for s in states:\n",
    "  if s not in end_actions:\n",
    "    i,j = s\n",
    "    possibles = []\n",
    "    if i>0 and (i-1,j) not in walls: possibles.append('U')\n",
    "    if j<w-1 and (i,j+1) not in walls: possibles.append('R')\n",
    "    if i<h-1 and (i+1,j) not in walls: possibles.append('D')\n",
    "    if j>0 and (i,j-1) not in walls: possibles.append('L')\n",
    "    actions[(i,j)]=possibles\n",
    "\n",
    "print(\"States\", states)\n",
    "print('\\nRewards:')\n",
    "for pos, r in rewards.items():\n",
    "  print(f'{pos}: {r}')\n",
    "print()\n",
    "print('Actions:')\n",
    "for pos, a in actions.items():\n",
    "  print(f'{pos}: {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocódigo del algoritmo de Iteración por valor\n",
    "\n",
    "<div align=\"right\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1JpOolQHTmkUxE9X5AclsU4JWCZ0aC_yq\"\n",
    "  width = 800\n",
    "  alt=\"Value Iteration Algorithm\">\n",
    "</div>\n",
    "\n",
    "`Pista`: Recuerde que para un entorno determinístico la ecuación de optimalidad de Bellman queda de la siguiente manera.\n",
    "\\begin{gather}\n",
    "  v^*(s)=\\max_{a}\\left[r+\\gamma v^*(s')\\right]\\\\\n",
    "  \\pi^*(s)=\\arg\\max_{a} v^*(s)\n",
    "\\end{gather}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(action, s):\n",
    "  i, j = s\n",
    "  if action   == 'U': i -= 1\n",
    "  elif action == 'R': j += 1\n",
    "  elif action == 'D': i += 1\n",
    "  elif action == 'L': j -= 1\n",
    "  return i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04  0.   -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.08]] \n",
      "\n",
      "Episode: 2\n",
      "[[-0.08 -0.08  0.96  1.  ]\n",
      " [-0.08  0.    0.92 -1.  ]\n",
      " [-0.08 -0.08  0.88  0.84]] \n",
      "\n",
      "Episode: 3\n",
      "[[-0.12  0.92  0.96  1.  ]\n",
      " [-0.12  0.    0.92 -1.  ]\n",
      " [-0.12  0.84  0.88  0.84]] \n",
      "\n",
      "Episode: 4\n",
      "[[ 0.88  0.92  0.96  1.  ]\n",
      " [ 0.84  0.    0.92 -1.  ]\n",
      " [ 0.8   0.84  0.88  0.84]] \n",
      "\n",
      "Política Óptima:\n",
      "[['R' 'R' 'R' '']\n",
      " ['U' '' 'U' '']\n",
      " ['U' 'R' 'U' 'L']]\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration Algorithm\n",
    "e = 1e-3\n",
    "episodes = 0\n",
    "\n",
    "# Initial Conditions\n",
    "V = np.array([[0. for j in range(w)] for i in range(h)])\n",
    "A = np.array([['' for j in range(w)] for i in range(h)])\n",
    "\n",
    "while True:\n",
    "  episodes += 1\n",
    "  old_V = V.copy()\n",
    "  d = 0\n",
    "  for s in states:\n",
    "    if s not in end_actions:\n",
    "      list_of_values, list_of_actions = [], []\n",
    "      for action in actions[s]:\n",
    "        sp = next_state(action, s)\n",
    "        list_of_values.append(rewards[s] + gamma*V[sp])\n",
    "        list_of_actions.append(action)\n",
    "      V[s] = np.max(list_of_values)\n",
    "      A[s] = list_of_actions[np.argmax(list_of_values)]\n",
    "    else:\n",
    "      V[s] = rewards[s]\n",
    "      A[s] = ''\n",
    "    if np.abs(V[s]-old_V[s]) > d: d = np.around(np.abs(V[s]-old_V[s]),3)\n",
    "  if d<=e*(1-gamma)/gamma: break\n",
    "  print(\"Episode:\", episodes)\n",
    "  print(np.around(V, 3), '\\n')\n",
    "\n",
    "print(\"Política Óptima:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado esperado\n",
    "\n",
    "<div align=\"right\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1DQRjO0Tx91LEBdDGPwQTBS5HB9oEl_W_\"\n",
    "  width = 600\n",
    "  alt=\"Check results\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
