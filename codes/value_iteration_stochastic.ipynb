{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3)]\n",
      "\n",
      "Rewards:\n",
      "(0, 0): -0.04\n",
      "(0, 1): -0.04\n",
      "(0, 2): -0.04\n",
      "(0, 3): 1\n",
      "(1, 0): -0.04\n",
      "(1, 2): -0.04\n",
      "(1, 3): -1\n",
      "(2, 0): -0.04\n",
      "(2, 1): -0.04\n",
      "(2, 2): -0.04\n",
      "(2, 3): -0.04\n",
      "\n",
      "Actions:\n",
      "(0, 0): ['R', 'D']\n",
      "(0, 1): ['R', 'L']\n",
      "(0, 2): ['R', 'D', 'L']\n",
      "(1, 0): ['U', 'D']\n",
      "(1, 2): ['U', 'R', 'D']\n",
      "(2, 0): ['U', 'R']\n",
      "(2, 1): ['R', 'L']\n",
      "(2, 2): ['U', 'R', 'L']\n",
      "(2, 3): ['U', 'L']\n"
     ]
    }
   ],
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "# Initial Conditions\n",
    "h, w = 3, 4\n",
    "gamma = 1\n",
    "r = -0.04\n",
    "\n",
    "# Enviroment, actions and rewards\n",
    "walls = [(1,1)]\n",
    "states = [(i,j) for i in range(h) for j in range(w) if (i,j) not in walls]\n",
    "rewards = {s:r for s in states}\n",
    "\n",
    "rewards[(0,3)] = 1\n",
    "rewards[(1,3)] = -1\n",
    "\n",
    "end_actions = [(0,3),(1,3)]\n",
    "\n",
    "actions = {}\n",
    "for s in states:\n",
    "  if s not in end_actions:\n",
    "    i,j = s\n",
    "    possibles = []\n",
    "    if i>0 and (i-1,j) not in walls: possibles.append('U')\n",
    "    if j<w-1 and (i,j+1) not in walls: possibles.append('R')\n",
    "    if i<h-1 and (i+1,j) not in walls: possibles.append('D')\n",
    "    if j>0 and (i,j-1) not in walls: possibles.append('L')\n",
    "    actions[(i,j)]=possibles\n",
    "    \n",
    "print(\"States:\", states)\n",
    "print('\\nRewards:')\n",
    "for s, r in rewards.items():\n",
    "  print(f'{s}: {r}')\n",
    "print()\n",
    "print('Actions:')\n",
    "for s, a in actions.items():\n",
    "  print(f'{s}: {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(s, action):\n",
    "  i, j = s\n",
    "  if action   == 'U': i -= 1\n",
    "  elif action == 'R': j += 1\n",
    "  elif action == 'D': i += 1\n",
    "  elif action == 'L': j -= 1\n",
    "  return i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_stochastic_state(s, action, actions):\n",
    "  i1, j1 = s\n",
    "  i2, j2 = s\n",
    "  if action == 'U' or action == 'D':\n",
    "    if 'L' in actions[s]: j1 -= 1\n",
    "    if 'R' in actions[s]: j2 += 1\n",
    "  else:\n",
    "    if 'U' in actions[s]: i1 -= 1\n",
    "    if 'D' in actions[s]: i2 += 1\n",
    "  return (i1,j1), (i2,j2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[ 0.812  0.868  0.918  1.   ]\n",
      " [ 0.762  0.     0.66  -1.   ]\n",
      " [ 0.705  0.655  0.611  0.388]]\n",
      "\n",
      " [['R' 'R' 'R' '']\n",
      " ['U' '' 'U' '']\n",
      " ['U' 'L' 'L' 'L']]\n",
      "episodes:  14\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration Algorithm\n",
    "e = 1e-3\n",
    "min_change = np.inf\n",
    "episodes = 0\n",
    "\n",
    "# Initial Conditions\n",
    "V = np.array([[0. for j in range(w)] for i in range(h)])\n",
    "A = np.array([['' for j in range(w)] for i in range(h)])\n",
    "p = 0.8\n",
    "\n",
    "while True:\n",
    "  episodes += 1\n",
    "  old_V = V.copy()\n",
    "  d = 0\n",
    "  for s in rewards:\n",
    "    if s not in end_actions:\n",
    "      list_of_values, list_of_actions = [], []\n",
    "      for action in actions[s]:\n",
    "        i,j = next_state(s, action)\n",
    "        value = p * (rewards[s] + gamma * V[i,j])\n",
    "        \n",
    "        (i1, j1), (i2, j2) = next_stochastic_state(s, action, actions)\n",
    "        value += (1-p)/2 * (rewards[s] + gamma * V[i1,j1])\n",
    "        value += (1-p)/2 * (rewards[s] + gamma * V[i2,j2])\n",
    "        \n",
    "        list_of_values.append(value)\n",
    "        list_of_actions.append(action)\n",
    "      V[s] = np.max(list_of_values)\n",
    "      A[s] = list_of_actions[np.argmax(list_of_values)]\n",
    "    else:\n",
    "      V[s] = rewards[s]\n",
    "      A[s] = ''\n",
    "    if np.abs(V[s]-old_V[s]) > d: d = np.around(np.abs(V[s]-old_V[s]),3)\n",
    "  if d<=e*(1-gamma)/gamma: break\n",
    "  \n",
    "print('\\n', np.around(V, 3))\n",
    "print('\\n', A)\n",
    "print('episodes: ', episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
